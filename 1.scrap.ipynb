{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis link:\n",
    "- https://medium.com/@naviubhi29/data-analysis-and-visualization-on-myanimelist-data-71129f499d7a\n",
    "- https://id.quora.com/Mengapa-genre-Slice-of-Life-banyak-yg-bilang-puncak-dari-wibu-Bukan-kah-itu-artinya-mulai-bosen-nonton-anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image\n",
    "import uuid  # Import UUID module\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapData(start_ ,end_):\n",
    "    for i in range(start_, end_, 50):      \n",
    "        # Cek apakah file data_{i}.json sudah ada\n",
    "        file_path = f'./dataset/data/data_{i}.json'\n",
    "        if os.path.exists(file_path):\n",
    "            # print(f\"Data for limit={i} already exists. Skipping... \")\n",
    "            continue\n",
    "        \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver_detail = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        driver.get(f\"https://myanimelist.net/topanime.php?limit={i}\")\n",
    "        \n",
    "        # Tunggu hingga elemen ranking-list muncul\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"ranking-list\"))\n",
    "            )\n",
    "        except:\n",
    "            # print(f\"No data found for limit={i}. Skipping...\")\n",
    "            driver.quit()\n",
    "            continue\n",
    "        \n",
    "        page = driver.page_source\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        list = soup.find_all('tr', class_='ranking-list')\n",
    "        \n",
    "        if not list:\n",
    "            # print(f\"No data found for limit={i}. Skipping...\")\n",
    "            driver.quit()\n",
    "            continue\n",
    "        \n",
    "        data = []\n",
    "\n",
    "        for idx, j in enumerate(list):\n",
    "            uuid_data = str(uuid.uuid4())\n",
    "            title = j.find('div', class_='di-ib').text\n",
    "            rank = j.find('span', class_='lightLink').text\n",
    "            link = j.find('a').get('href')\n",
    "            mal_id = link.split('/')[4]\n",
    "            \n",
    "            anime_data = {\n",
    "                'id': uuid_data,\n",
    "                'mal_id': mal_id,\n",
    "                'title': title,\n",
    "                'image_url': '',\n",
    "                'synopsis': '',\n",
    "                'aired': '',\n",
    "                'premiered': '',\n",
    "                'member': '',\n",
    "                'favorite': '',\n",
    "                'rank': rank,\n",
    "                'link': link,\n",
    "                'episode': 0,\n",
    "                'type': '',\n",
    "                'genre': [],\n",
    "                'producer': [],\n",
    "                'studio': [],\n",
    "                'theme': [],\n",
    "                'demographic': [],\n",
    "                'duration': '',\n",
    "                'rating': '',\n",
    "            }\n",
    "            \n",
    "            # # Detail \n",
    "            driver_detail.get(link)\n",
    "            page_detail = driver_detail.page_source\n",
    "            soup_detail = BeautifulSoup(page_detail, 'html.parser')\n",
    "            \n",
    "            # Image\n",
    "            image_url_ = soup_detail.find('img', itemprop='image')\n",
    "            if image_url_:\n",
    "                anime_data['image_url'] =  image_url_['data-src']\n",
    "            \n",
    "            \n",
    "            # Genre\n",
    "            genre_list = soup_detail.find_all('span', itemprop='genre')\n",
    "            if genre_list:\n",
    "                anime_data['genre'] = [g.text for g in genre_list]\n",
    "            \n",
    "            # Synopsis\n",
    "            synopsis_ = soup_detail.find('p', itemprop='description')\n",
    "            if synopsis_:\n",
    "                cleaned_synopsis = synopsis_.text.replace('\\n', ' ')\n",
    "                anime_data['synopsis'] = cleaned_synopsis\n",
    "                \n",
    "            # Producer\n",
    "            producer_ = soup_detail.find('span', class_='dark_text', text='Producers:')\n",
    "            if producer_:\n",
    "                producers = [a.text for a in producer_.find_next_siblings('a')]\n",
    "                anime_data['producer'] = producers\n",
    "            \n",
    "            # Studio\n",
    "            studio_ = soup_detail.find('span', class_='dark_text', text='Studios:')\n",
    "            if studio_:\n",
    "                studios = [a.text for a in studio_.find_next_siblings('a')]\n",
    "                anime_data['studio'] = studios\n",
    "\n",
    "            # Theme\n",
    "            theme_ = soup_detail.find('span', class_='dark_text', text='Theme:')\n",
    "            if theme_:\n",
    "                theme_ = [a.text for a in theme_.find_next_siblings('a')]\n",
    "                anime_data['theme'] = theme_\n",
    "\n",
    "            # Demographic\n",
    "            demographic_ = soup_detail.find('span', class_='dark_text', text='Demographic:')\n",
    "            if demographic_:\n",
    "                demographic_ = [a.text for a in demographic_.find_next_siblings('a')]\n",
    "                anime_data['demographic'] = demographic_\n",
    "\n",
    "            # Episode\n",
    "            episode_ = soup_detail.find('span', class_='dark_text', text='Episodes:')\n",
    "            if episode_:\n",
    "                anime_data['episode'] = episode_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "            # Type\n",
    "            type_ = soup_detail.find('span', class_='dark_text', text='Type:')\n",
    "            if type_:\n",
    "                anime_data['type'] = type_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "            # Duration\n",
    "            duration_ = soup_detail.find('span', class_='dark_text', text='Duration:')\n",
    "            if duration_:\n",
    "                anime_data['duration'] = duration_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\")\n",
    "\n",
    "            # Rating\n",
    "            rating_ = soup_detail.find('span', class_='dark_text', text='Rating:')\n",
    "            if rating_:\n",
    "                anime_data['rating'] = rating_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "                \n",
    "            # Aired\n",
    "            aired_ = soup_detail.find('span', class_='dark_text', text='Aired:')\n",
    "            if aired_:\n",
    "                anime_data['aired'] = aired_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "                \n",
    "            # Premiered\n",
    "            premiered_ = soup_detail.find('span', class_='dark_text', text='Premiered:')\n",
    "            if premiered_:\n",
    "                anime_data['premiered'] = premiered_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "            # Member\n",
    "            member_ = soup_detail.find('span', class_='dark_text', text='Member:')\n",
    "            if member_:\n",
    "                anime_data['member'] = member_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "            # Favorite\n",
    "            favorite_ = soup_detail.find('span', class_='dark_text', text='Favorites:')\n",
    "            if favorite_:\n",
    "                anime_data['favorite'] = favorite_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "                \n",
    "\n",
    "            data.append(anime_data) \n",
    "\n",
    "            # Print progress\n",
    "            progress = (idx + 1) / len(list) * 100\n",
    "            print(f\"\\rScraping Data ke {i} [{'=' * int(progress // 10)}{' ' * (10 - int(progress // 10))}] {idx + 1}/{len(list)}\", end=\"\", flush=True)\n",
    "        print()\n",
    "        \n",
    "        # Save data \n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        \n",
    "        print(data)\n",
    "        \n",
    "        driver.quit()\n",
    "    \n",
    "    return print(f\"Data Scrap {start_} to {end_} Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scrap 0 to 5050 Completed\n"
     ]
    }
   ],
   "source": [
    "scrapData(0, 5050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scrap 5050 to 10050 Completed\n"
     ]
    }
   ],
   "source": [
    "scrapData(5050, 10050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scrap 10050 to 15050 Completed\n"
     ]
    }
   ],
   "source": [
    "scrapData(10050, 15050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scrap 15050 to 20050 Completed\n"
     ]
    }
   ],
   "source": [
    "scrapData(15050, 20050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scrap 20050 to 28000 Completed\n"
     ]
    }
   ],
   "source": [
    "scrapData(20050, 28050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# for idx in range(1, 50):  # Contoh loop dari 1 hingga 100\n",
    "#     # Progress bar yang direplace\n",
    "#     progress = (idx + 1) / 50 * 100\n",
    "#     print(f\"\\r Scraping Data ke {i} [{'=' * int(progress // 10)}{' ' * (10 - int(progress // 10))}] {idx + 1}/{50}\", end=\"\", flush=True)\n",
    "#     time.sleep(0.2)  # Simulasi delay\n",
    "# print()  # Pindah ke baris baru setelah selesai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "# options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "# driver.get(f\"https://myanimelist.net/topanime.php?limit=100\")\n",
    "# page = driver.page_source\n",
    "# soup = BeautifulSoup(page, 'html.parser')\n",
    "# list = soup.find_all('tr', class_='ranking-list')\n",
    "# data = []\n",
    "\n",
    "# for i in list:\n",
    "#     uuid_data = str(uuid.uuid4())\n",
    "#     title = i.find('div', class_='di-ib').text\n",
    "#     rank = i.find('span', class_='lightLink').text\n",
    "#     link = i.find('a').get('href')\n",
    "#     mal_id = link.split('/')[4]\n",
    "    \n",
    "#     anime_data = {\n",
    "#         'id': uuid_data,\n",
    "#         'mal_id': mal_id,\n",
    "#         'title': title,\n",
    "#         'rank': rank,\n",
    "#         'link': link,\n",
    "#         'synopsis': '',\n",
    "#         'episode': 0,\n",
    "#         'type': '',\n",
    "#         'genre': [],\n",
    "#         'producer': [],\n",
    "#         'studio': [],\n",
    "#         'theme': [],\n",
    "#         'demographic': [],\n",
    "#         'duration': '',\n",
    "#         'rating': '',\n",
    "#     }\n",
    "    \n",
    "#     # # Detail \n",
    "#     driver.get(link)\n",
    "#     page_detail = driver.page_source\n",
    "#     soup_detail = BeautifulSoup(page_detail, 'html.parser')\n",
    "    \n",
    "#     # Synopsis\n",
    "#     synopsis_ = soup_detail.find('p', itemprop='description')\n",
    "#     if synopsis_.text:\n",
    "#         cleaned_synopsis = synopsis_.text.replace('\\n', ' ')\n",
    "#         anime_data['synopsis'] = cleaned_synopsis\n",
    "    \n",
    "#     # Genre\n",
    "#     genre_list = soup_detail.find_all('span', itemprop='genre')\n",
    "#     if genre_list:\n",
    "#         anime_data['genre'] = [g.text for g in genre_list]\n",
    "    \n",
    "#     # Producer\n",
    "#     producer_ = soup_detail.find('span', class_='dark_text', text='Producers:')\n",
    "#     if producer_:\n",
    "#         producers = [a.text for a in producer_.find_next_siblings('a')]\n",
    "#         anime_data['producer'] = producers\n",
    "    \n",
    "#     # Studio\n",
    "#     studio_ = soup_detail.find('span', class_='dark_text', text='Studios:')\n",
    "#     if studio_:\n",
    "#         studios = [a.text for a in studio_.find_next_siblings('a')]\n",
    "#         anime_data['studio'] = studios\n",
    "\n",
    "#     # Theme\n",
    "#     theme_ = soup_detail.find('span', class_='dark_text', text='Theme:')\n",
    "#     if theme_:\n",
    "#         theme_ = [a.text for a in theme_.find_next_siblings('a')]\n",
    "#         anime_data['theme'] = theme_\n",
    "\n",
    "#     # Demographic\n",
    "#     demographic_ = soup_detail.find('span', class_='dark_text', text='Demographic:')\n",
    "#     if demographic_:\n",
    "#         demographic_ = [a.text for a in demographic_.find_next_siblings('a')]\n",
    "#         anime_data['demographic'] = demographic_\n",
    "\n",
    "#     # # Episode\n",
    "#     episode_ = soup_detail.find('span', class_='dark_text', text='Episodes:')\n",
    "#     if episode_:\n",
    "#         anime_data['episode'] = episode_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "#     # Type\n",
    "#     type_ = soup_detail.find('span', class_='dark_text', text='Type:')\n",
    "#     if type_:\n",
    "#         anime_data['type'] = type_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "#     # Duration\n",
    "#     duration_ = soup_detail.find('span', class_='dark_text', text='Duration:')\n",
    "#     if duration_:\n",
    "#         anime_data['duration'] = duration_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\")\n",
    "\n",
    "#     # Rating\n",
    "#     rating_ = soup_detail.find('span', class_='dark_text', text='Rating:')\n",
    "#     if rating_:\n",
    "#         anime_data['rating'] = rating_.find_parent().text.strip().split(':', 1)[1].replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "    \n",
    "\n",
    "#     data.append(anime_data)\n",
    "    \n",
    "# print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
